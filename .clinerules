# QueryForge Project Rules

## Query Building - MANDATORY

**CRITICAL**: When building queries for any security platform (CBC, Cortex, KQL, S1):

### 1. ALWAYS Use MCP Query Builder Tools

You MUST use the appropriate MCP query builder tool for the platform:

- **Carbon Black Cloud**: `cbc_build_query`
- **Cortex XDR**: `cortex_build_query`
- **Microsoft Defender/KQL**: `kql_build_query`
- **SentinelOne**: `s1_build_query`

### 2. NEVER Manually Write Query Syntax

Do NOT manually construct query strings. The query builders:
- Use the correct field schema from actual data sources
- Apply proper operator normalization from schema definitions
- Handle value formatting and escaping correctly
- Generate syntactically valid platform-specific queries

### 3. Correct Field Schema Usage

The query builders ensure correct field names are used:
- **SentinelOne**: `src.process.name` (NOT `SrcProcName`)
- **Cortex XDR**: `actor_process_image_name` (NOT `ActorProcessImageName`)
- **KQL**: Table-specific column names
- **CBC**: Documented field names from schema

### 4. Query Building Parameters

Use the appropriate parameters when calling build tools:

- **natural_language_intent**: When user describes what to find in plain language
- **filters**: For structured field/operator/value conditions
- **dataset/table/search_type**: To specify the data source
- **time_range/time_window**: For temporal filtering

## Examples

### ✅ CORRECT Approach
```
Use s1_build_query tool:
{
  "dataset": "files",
  "natural_language_intent": "browser downloads from chrome or firefox"
}
```

### ❌ INCORRECT Approach
```
Manually writing:
"SrcProcName = 'chrome.exe'" 
# WRONG! This uses incorrect field schema in S1 and bypasses the query builder
```

## Why This Matters

1. **Field Schema Accuracy**: Platforms update their schemas; the query builder uses the current schema
2. **Operator Validation**: Each platform has specific operators; the builder validates and normalizes them
3. **Value Escaping**: Different platforms require different escaping (quotes, backslashes, etc.)
4. **Syntax Correctness**: Platform-specific syntax rules are enforced by the builder

## Query Development Workflow

### RECOMMENDED: Use Optimized Combined Build+Validate Tools

For best performance, use the new combined build+validate tools that automatically handle validation and retries:

- **Carbon Black Cloud**: `cbc_build_query_validated`
- **Cortex XDR**: `cortex_build_query_validated`
- **Microsoft Defender/KQL**: `kql_build_query_validated`
- **SentinelOne**: `s1_build_query_validated`

**Benefits:**
- ✅ **10x faster** - Single API call instead of build → validate → rebuild → revalidate cycles
- ✅ **Automatic corrections** - Built-in retry logic applies fixes without LLM reasoning delays
- ✅ **Validation included** - Returns both the query and validation results
- ✅ **Caching enabled** - Repeated/similar queries validate instantly from cache

**Example Usage:**
```
result = call_tool("s1_build_query_validated", {
  "dataset": "processes",
  "natural_language_intent": "chrome processes"
})

# Result includes:
# - result["query"]: Validated query string
# - result["validation"]: Full validation results
# - result["retry_count"]: Number of corrections applied (0 if perfect first try)
# - result["corrections_applied"]: List of automatic fixes
```

### Alternative: Traditional Two-Step Workflow

If you need fine-grained control, you can use the traditional approach:

1. Understand the user's intent
2. Identify the appropriate platform and dataset
3. Use the corresponding MCP `*_build_query` tool
4. **MANDATORY: Validate the query** using the corresponding `*_validate_query` tool
5. **If validation fails (valid=False)**: Fix and retry (see validation workflow below)
6. Present the validated query to the user

Never skip step 3 by manually writing queries!
**Never skip step 4 - validation is MANDATORY!**

## Query Validation - MANDATORY

### CRITICAL: Always Validate Queries Before Presenting to Users

After building a query with any `*_build_query` tool, you MUST:

1. **Immediately call the corresponding validation tool**:
   - `cbc_build_query` → `cbc_validate_query`
   - `cortex_build_query` → `cortex_validate_query`
   - `kql_build_query` → `kql_validate_query`
   - `s1_build_query` → `s1_validate_query`

2. **Pass both the query and metadata to validation**:
   ```
   # Build result includes both query and metadata
   build_result = call_tool("s1_build_query", {...})

   # Pass both to validation
   validate_result = call_tool("s1_validate_query", {
       "query": build_result["query"],
       "metadata": build_result["metadata"]
   })
   ```

3. **Check the validation result**:
   - If `valid=True`: Safe to present the query to the user
   - If `valid=False`: **You MUST fix the query before presenting it**

### Validation Failure Retry Workflow - REQUIRED

When `valid=False` (validation fails), you MUST:

1. **Read ALL errors** in `validation_results`:
   ```json
   {
     "valid": false,
     "validation_results": {
       "syntax": {"errors": [...]},
       "schema": {"errors": [...]},
       "operators": {"errors": [...]},
       "performance": {"errors": [...]},
       "best_practices": {"errors": [...]}
     }
   }
   ```

2. **Extract suggestions from each error**:
   - Each error has a `suggestion` field with actionable guidance
   - Example: "Field 'process_name' not found. Did you mean: tgt.process.name?"

3. **Call the build tool again with corrections**:
   - Fix field names based on suggestions
   - Adjust dataset/table if needed
   - Correct operators as suggested

4. **Validate the corrected query**:
   - Call the validate tool again on the new query

5. **Repeat until valid=True**:
   - Continue fixing and validating until the query passes
   - Do NOT give up after one retry - keep fixing until valid

6. **NEVER present invalid queries to users**:
   - Invalid queries may fail at runtime
   - They may return incorrect results
   - They may have security or performance issues

### Example Retry Pattern

```python
# Initial build
result = call_tool("s1_build_query", {
    "dataset": "processes",
    "filters": [{"field": "process_name", "value": "cmd.exe"}]
})

# Validate
validation = call_tool("s1_validate_query", {
    "query": result["query"],
    "metadata": result["metadata"]
})

# Check if valid
if not validation["valid"]:
    # Extract error
    error = validation["validation_results"]["schema"]["errors"][0]
    # error["message"]: "Field 'process_name' not found"
    # error["suggestion"]: "Did you mean: tgt.process.name?"

    # Retry with correction
    result = call_tool("s1_build_query", {
        "dataset": "processes",
        "filters": [{"field": "tgt.process.name", "value": "cmd.exe"}]
    })

    # Validate again
    validation = call_tool("s1_validate_query", {
        "query": result["query"],
        "metadata": result["metadata"]
    })
    # Now validation["valid"] should be True
```

### Handling Warnings

- **Warnings** (when `valid=True` but warnings exist) should be shown to the user
- Warnings indicate potential performance or best practice issues
- The query is valid but may benefit from optimization
- Present warnings to the user along with the query

### Summary

**MANDATORY RULES**:
1. ✅ ALWAYS validate after building
2. ✅ ALWAYS retry with corrections when valid=False
3. ✅ ALWAYS repeat until valid=True
4. ❌ NEVER present invalid queries to users
5. ❌ NEVER skip validation
6. ❌ NEVER give up after one retry - keep fixing until valid
